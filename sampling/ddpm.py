import matplotlib.pyplot as plt
import os
import torch
import torchvision

# INFERENCE
@torch.no_grad() # Decorator to disable gradient calculations for the whole function
def generate_samples(model,              # The U-Net model
                     fdp,                # ForwardDiffusionProcess instance
                     T,                  # Total timesteps
                     epoch,              # Current epoch number (for titles/filenames)
                     device,             # Device ('cuda' or 'cpu')
                     initial_noise=None,  # Optionally pass a fixed init noise
                     num_images=16,      # How many images to generate
                     img_size=28,
                     img_channels=1,
                     save_dir="/nvme/notebooks/samples",
                     seed=42): # Directory to save samples
    """
    Generates images using the reverse diffusion process, displays them,
    and saves them to a file. Operates in eval mode and without gradients.
    Resets RNG seed internally for deterministic output given same inputs.
    """
    # 1. Set model to evaluation mode
    model.eval()

    # 2. Use provided noise or Generate Initial Noise
    if initial_noise is not None:
        if initial_noise.shape[0] != num_images:
             print(f"Warning: num_images ({num_images}) differs from initial_noise batch size ({initial_noise.shape[0]}). Using noise's batch size.")
             num_images = initial_noise.shape[0] # Adjust num_images to match provided noise
        # Use a clone to avoid modifying the original cached noise tensor
        x_t = initial_noise.clone().to(device)
    else:
        # Generate new random noise if none provided
        torch.manual_seed(seed)
        x_t = torch.randn(num_images, img_channels, img_size, img_size, device=device)

    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        
    # 3. Sampling Loop (Reverse Diffusion)
    sampling_iterator = tqdm(range(T - 1, -1, -1), desc="Generating Samples", leave=False)
    for t_ in sampling_iterator:
        # Create tensor of current timestep for the whole batch
        t = torch.full((num_images,), t_, device=device, dtype=torch.long)

        # Predict noise using the U-Net model
        predicted_noise = model(x_t, t)
        padded_predicted_noise = pad_output_to_target(predicted_noise, epsilon)

        # Get the less noisy image x_{t-1} using the diffusion process sampler
        x_t = fdp.p_sample(padded_predicted_noise, x_t, t) # x_t is updated

    # 4. Process Output Images (final x_t is x_0)
    generated_images = x_t
    generated_images = generated_images * 0.5 + 0.5 # Denormalize [-1, 1] -> [0, 1]
    generated_images.clamp_(0, 1) # Clamp to valid range
    generated_images = generated_images.cpu() # Move to CPU for plotting/saving

    # 5. Create Grid
    grid = torchvision.utils.make_grid(generated_images, nrow=int(num_images**0.5)) # Adjust nrow (e.g., 4 for 16 images)

    # 6. Display Grid
    plt.figure(figsize=(8, 8))
    plt.imshow(grid.permute(1, 2, 0)) # Adjust dimensions for imshow (C, H, W) -> (H, W, C)
    plt.title(f"Generated MNIST Digits - Epoch {epoch+1} Batch {BATCH_Size} LR {LEARNING_RATE}")
    plt.axis('off')

    # Save the entire figure generated by Matplotlib
    if save_dir:
        try:
            os.makedirs(save_dir, exist_ok=True)
            output_filename = os.path.join(save_dir, f"mnist_run_{RUN_NUMBER}_epoch_{epoch+1:04d}_seed{seed}.png")
            # Use plt.savefig BEFORE plt.show()
            plt.savefig(output_filename, bbox_inches='tight', pad_inches=0.1) # Save the figure
            print(f"Generated plot saved to {output_filename}")
        except Exception as e:
            print(f"Error saving plot: {e}")

    # Preview sample figure in output
    plt.show()